{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imperceptible attack on tabular data using LowProFool algorithm\n",
    "In this notebook, we will learn how to execute imperceptible attack on tabular data with the LowProFool algorithm (https://arxiv.org/abs/1911.03274). We will use **iris flowers** and **breast cancer** datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification.scikitlearn import ScikitlearnLogisticRegression\n",
    "from art.estimators.classification.pytorch import PyTorchClassifier\n",
    "from art.attacks.evasion import LowProFool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Data preparation\n",
    "\n",
    "Firstly, we load the datasets, standardize them, and split into training and validation sets. We also choose the clipping values for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    \"\"\"\n",
    "    Get both the standardized data and the used scaler.\n",
    "    \"\"\"\n",
    "    columns = data.columns\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    return pd.DataFrame(data=x_scaled, columns=columns), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "def get_train_and_valid(design_matrix, labels):\n",
    "    \"\"\"\n",
    "    Split dataset into training and validation sets.\n",
    "    \"\"\"\n",
    "    for train_idx, valid_idx in split.split(design_matrix, labels):\n",
    "        X_train = design_matrix.iloc[train_idx].copy()\n",
    "        X_valid = design_matrix.iloc[valid_idx].copy()\n",
    "        y_train = labels.iloc[train_idx].copy()\n",
    "        y_valid = labels.iloc[valid_idx].copy()\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preparation of the iris flowers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "design_matrix_iris = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "labels_iris = pd.Series(data=iris['target'])\n",
    "display(design_matrix_iris)\n",
    "\n",
    "design_matrix_iris_scaled, iris_scaler = standardize(design_matrix_iris)\n",
    "\n",
    "X_train_iris, y_train_iris, X_valid_iris, y_valid_iris =\\\n",
    "    get_train_and_valid(design_matrix_iris_scaled, labels_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preparation of the breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()\n",
    "design_matrix_cancer = pd.DataFrame(data=cancer['data'], columns=cancer['feature_names'])\n",
    "labels_cancer = pd.Series(data=cancer['target'])\n",
    "display(design_matrix_cancer)\n",
    "\n",
    "design_matrix_cancer_scaled, cancer_scaler = standardize(design_matrix_cancer)\n",
    "\n",
    "X_train_cancer, y_train_cancer, X_valid_cancer, y_valid_cancer =\\\n",
    "    get_train_and_valid(design_matrix_cancer_scaled, labels_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris flowers dataset** - minimum and maximum values in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_clip_values_iris = (\n",
    "    np.array(X_train_iris.min()),\n",
    "    np.array(X_train_iris.max())\n",
    ")\n",
    "print(\"Clip-values:\")\n",
    "print(\"  Lower bound:\", scaled_clip_values_iris[0])\n",
    "print(\"  Upper bound:\", scaled_clip_values_iris[1])\n",
    "\n",
    "print(\"\\nClip-values in original scale:\")\n",
    "clip_values_iris = iris_scaler.inverse_transform(scaled_clip_values_iris)\n",
    "print(\"  Lower bound:\", clip_values_iris[0])\n",
    "print(\"  Upper bound:\", clip_values_iris[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breast cancer dataset** - 1 standard deviation boundary.\n",
    "\n",
    "Note: Here, we create clip values such that all values should fall within the one standard deviation interval. Thanks to the dataset being priorly standardized, it is a trivial problem. Moreover clip values can be concisely expressed as just a single tuple `(-1., 1.)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_clip_values_cancer = (-1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Quick LowProFool example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kickoff example using LowProFool to generate adversaries for SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# import ART wrapper for scikit-learn SVC\n",
    "from art.estimators.classification.scikitlearn import ScikitlearnSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit SVC\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(X_train_cancer.values, y_train_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap SVC using the wrapper\n",
    "classif_svc = ScikitlearnSVC(\n",
    "    model = svc_clf,\n",
    "    clip_values = scaled_clip_values_cancer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LowProFool instance and fit feature importance\n",
    "lpf_svc = LowProFool(\n",
    "    classifier = classif_svc,\n",
    "    n_steps    = 15,\n",
    "    eta        = 15,\n",
    "    lambd      = 1.75,\n",
    "    eta_decay  = 0.985,\n",
    "    verbose    = True\n",
    ")\n",
    "lpf_svc.fit_importances(X_train_cancer, y_train_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random array of samples to be used as adveraries\n",
    "n_classes = lpf_svc.n_classes\n",
    "targets = np.eye(n_classes)[np.array(\n",
    "    y_valid_cancer.apply(lambda x: np.random.choice([i for i in range(n_classes) if i != x]))\n",
    ")]\n",
    "\n",
    "# Generate adversaries\n",
    "adversaries = lpf_svc.generate(x=X_valid_cancer, y=targets)\n",
    "\n",
    "# Check the success rate\n",
    "expected = np.argmax(targets, axis=1)\n",
    "predicted = np.argmax(classif_svc.predict(adversaries), axis=1)\n",
    "\n",
    "correct = (expected == predicted)\n",
    "success_rate = np.sum(correct) / correct.shape[0]\n",
    "print(\"Test set size: {}\".format(targets.shape[0]))\n",
    "print(\"Success rate:  {:.2f}%\".format(100*success_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, one can easily generate good quality adversary examples in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Extended LowProFool example\n",
    "\n",
    "In this section we present you few more examples of `LowProFool` adversarial attacks carried out in a similar fashion, but employing different underlying models and on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on iris flowers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regression_clf_iris = LogisticRegression()\n",
    "log_regression_clf_iris.fit(X_train_iris.values, y_train_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regression_clf_cancer = LogisticRegression()\n",
    "log_regression_clf_cancer.fit(X_train_cancer.values, y_train_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model(input_dimensions, hidden_neurons, output_dimensions):\n",
    "    \"\"\"\n",
    "    Prepare PyTorch neural network.\n",
    "    \"\"\"\n",
    "    return torch.nn.Sequential(\n",
    "        nn.Linear(input_dimensions, hidden_neurons),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_neurons, output_dimensions),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "def train_nn(nn_model, X, y, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Train provided neural network.\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        y_pred = nn_model.forward(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        nn_model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on iris flowers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.FloatTensor(np.array(X_train_iris)))\n",
    "y = Variable(torch.FloatTensor(np.eye(3)[y_train_iris]))\n",
    "nn_model_iris = get_nn_model(4, 10, 3)\n",
    "train_nn(nn_model_iris, X, y, 1e-4, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.FloatTensor(np.array(X_train_cancer.values)))\n",
    "y = Variable(torch.FloatTensor(np.eye(2)[y_train_cancer]))\n",
    "nn_model_cancer = get_nn_model(30, 50, 2)\n",
    "train_nn(nn_model_cancer, X, y, 1e-4, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual usage of LowProFool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowprofool_generate_adversaries_test_lr(lowprofool, classifier, x_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Testing utility.\n",
    "    \"\"\"\n",
    "    n_classes = lowprofool.n_classes\n",
    "    \n",
    "    # Generate targets\n",
    "    target = np.eye(n_classes)[np.array(\n",
    "        y_valid.apply(\n",
    "            lambda x: np.random.choice([i for i in range(n_classes) if i != x]))\n",
    "    )]\n",
    "    \n",
    "    # Generate adversaries\n",
    "    adversaries = lowprofool.generate(x=x_valid, y=target)\n",
    "\n",
    "    # Test - check the success rate\n",
    "    expected = np.argmax(target, axis=1)\n",
    "    predicted = np.argmax(classifier.predict_proba(adversaries), axis=1)\n",
    "    correct = (expected == predicted)\n",
    "    \n",
    "    success_rate = np.sum(correct) / correct.shape[0]\n",
    "    print(\"Success rate: {:.2f}%\".format(100*success_rate))\n",
    "    \n",
    "    return adversaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris flowers dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping classifier into appropriate ART-friendly wrapper\n",
    "logistic_regression_iris_wrapper = ScikitlearnLogisticRegression(\n",
    "    model       = log_regression_clf_iris, \n",
    "    clip_values = scaled_clip_values_iris\n",
    ")\n",
    "\n",
    "# Creating LowProFool instance\n",
    "lpf_logistic_regression_iris = LowProFool(\n",
    "    classifier = logistic_regression_iris_wrapper, \n",
    "    eta        = 5,\n",
    "    lambd      = 0.2, \n",
    "    eta_decay  = 0.9\n",
    ")\n",
    "\n",
    "# Fitting feature importance\n",
    "lpf_logistic_regression_iris.fit_importances(X_train_iris, y_train_iris)\n",
    "\n",
    "# Testing\n",
    "results_lr_ir = lowprofool_generate_adversaries_test_lr(\n",
    "    lowprofool = lpf_logistic_regression_iris,\n",
    "    classifier = log_regression_clf_iris, \n",
    "    x_valid    = X_valid_iris, \n",
    "    y_valid    = y_valid_iris\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful adversarial attack. Below we can see the original features and their classes, as well as the adversaries generated by `LowProFool` and predicted class-wise probabilities of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(values, preds, max_features=4):\n",
    "    \"\"\"\n",
    "    Utility function for printing predictions.\n",
    "    \"\"\"\n",
    "    predictions = zip(list(map(lambda e: e[:max_features], values.tolist())), preds.tolist())\n",
    "    \n",
    "    for features, pred in predictions:\n",
    "        print(\"Features[:{}]:\".format(max_features))\n",
    "        for i, val in enumerate(features):\n",
    "            if i % 6 != 5: print(\"{:>10.4f}\".format(val), end='')\n",
    "            else:          print(\"{:>10.4f}\\n\".format(val), end='')\n",
    "        if len(features) % 6 != 0: print()\n",
    "        \n",
    "        print(\"Prediction (probability -> class):\")\n",
    "        for val in pred:\n",
    "            print(\"{:>8.3f}\".format(val), end='')\n",
    "        print(\"  ->  {}\\n\".format(np.argmax(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Original values ===\\n\")\n",
    "\n",
    "print_predictions(iris_scaler.inverse_transform(X_valid_iris[-3:].values), \n",
    "    log_regression_clf_iris.predict_proba(X_valid_iris[-3:]))\n",
    "    \n",
    "print(\"\\n=== Adversaries (LowProFool results) ===\\n\")\n",
    "    \n",
    "print_predictions(iris_scaler.inverse_transform(results_lr_ir[-3:]), \n",
    "    log_regression_clf_iris.predict_proba(results_lr_ir[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breast cancer dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping classifier into appropriate ART-friendly wrapper\n",
    "logistic_regression_cancer_wrapper = ScikitlearnLogisticRegression(\n",
    "    model       = log_regression_clf_cancer, \n",
    "    clip_values = scaled_clip_values_cancer\n",
    ")\n",
    "\n",
    "# Creating LowProFool instance\n",
    "lpf_logistic_regression_cancer = LowProFool(\n",
    "    classifier = logistic_regression_cancer_wrapper, \n",
    "    eta        = 5,\n",
    "    lambd      = 0.2, \n",
    "    eta_decay  = 0.9\n",
    ")\n",
    "\n",
    "# Fitting feature importance\n",
    "lpf_logistic_regression_cancer.fit_importances(X_train_cancer.values, y_train_cancer)\n",
    "\n",
    "# Testing\n",
    "results_lr_bc = lowprofool_generate_adversaries_test_lr(\n",
    "    lowprofool = lpf_logistic_regression_cancer,\n",
    "    classifier = log_regression_clf_cancer, \n",
    "    x_valid    = X_valid_cancer, \n",
    "    y_valid    = y_valid_cancer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Original values ===\\n\")\n",
    "\n",
    "print_predictions(cancer_scaler.inverse_transform(X_valid_cancer[-2:]), \n",
    "                  log_regression_clf_cancer.predict_proba(X_valid_cancer[-2:]), max_features=30)\n",
    "\n",
    "print(\"\\n=== Adversaries (LowProFool results) ===\\n\")\n",
    "\n",
    "print_predictions(cancer_scaler.inverse_transform(results_lr_bc[-2:]), \n",
    "                  log_regression_clf_cancer.predict_proba(results_lr_bc[-2:]), max_features=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowprofool_generate_adversaries_test_nn(lowprofool, classifier, x_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Testing utility.\n",
    "    \"\"\"\n",
    "    n_classes = lowprofool.n_classes\n",
    "    \n",
    "    # Generate targets\n",
    "    target = np.eye(n_classes)[np.array(\n",
    "        y_valid.apply(\n",
    "            lambda x: np.random.choice([i for i in range(n_classes) if i != x]))\n",
    "    )]\n",
    "    \n",
    "    # Generate adversaries\n",
    "    adversaries = lowprofool.generate(x=x_valid, y=target)\n",
    "\n",
    "    # Test - check the success rate\n",
    "    expected = np.argmax(target, axis=1)\n",
    "    x = Variable(torch.from_numpy(adversaries.astype(np.float32)))\n",
    "    predicted = np.argmax(classifier.forward(x).detach().numpy(), axis=1)\n",
    "    correct = (expected == predicted)\n",
    "    \n",
    "    success_rate = np.sum(correct) / correct.shape[0]\n",
    "    print(\"Success rate: {:.2f}%\".format(100*success_rate))\n",
    "    \n",
    "    return adversaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris flowers dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping classifier into appropriate ART-friendly wrapper\n",
    "# (in this case it is PyTorch NN classifier wrapper from ART)\n",
    "neural_network_iris_wrapper = PyTorchClassifier(\n",
    "    model       = nn_model_iris, \n",
    "    loss        = loss_fn,\n",
    "    input_shape = (4,),\n",
    "    nb_classes  = 3,\n",
    "    clip_values = scaled_clip_values_iris\n",
    ")\n",
    "\n",
    "# Creating LowProFool instance\n",
    "lpf_neural_network_iris = LowProFool(\n",
    "    classifier = neural_network_iris_wrapper,\n",
    "    n_steps    = 100,\n",
    "    eta        = 7,\n",
    "    lambd      = 1.75, \n",
    "    eta_decay  = 0.95\n",
    ")\n",
    "\n",
    "# Fitting feature importance\n",
    "lpf_neural_network_iris.fit_importances(X_train_iris, y_train_iris)\n",
    "\n",
    "# Testing\n",
    "results_nn_ir = lowprofool_generate_adversaries_test_nn(\n",
    "    lowprofool = lpf_neural_network_iris,\n",
    "    classifier = nn_model_iris, \n",
    "    x_valid    = X_valid_iris, \n",
    "    y_valid    = y_valid_iris\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"=== Original values ===\\n\")\n",
    "\n",
    "print_predictions(iris_scaler.inverse_transform(X_valid_iris[:3].values),\n",
    "      neural_network_iris_wrapper.predict(X_valid_iris[:3].values.astype(np.float32)))\n",
    "\n",
    "print(\"\\n=== Adversaries (LowProFool results) ===\\n\")\n",
    "\n",
    "print_predictions(iris_scaler.inverse_transform(results_nn_ir[:3]), \n",
    "      neural_network_iris_wrapper.predict(results_nn_ir.astype(np.float32)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breast cancer dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping classifier into appropriate ART-friendly wrapper\n",
    "# (in this case it is PyTorch NN classifier wrapper from ART)\n",
    "neural_network_cancer_wrapper = PyTorchClassifier(\n",
    "    model       = nn_model_cancer, \n",
    "    loss        = loss_fn, \n",
    "    input_shape = (30,),\n",
    "    nb_classes  = 2,\n",
    "    clip_values = scaled_clip_values_cancer\n",
    ")\n",
    "\n",
    "# Creating LowProFool instance\n",
    "lpf_neural_network_cancer = LowProFool(\n",
    "    classifier = neural_network_cancer_wrapper,\n",
    "    n_steps    = 200,\n",
    "    eta        = 10,\n",
    "    lambd      = 2, \n",
    "    eta_decay  = 0.99\n",
    ")\n",
    "\n",
    "# Fitting feature importance\n",
    "lpf_neural_network_cancer.fit_importances(X_train_cancer, y_train_cancer)\n",
    "\n",
    "# Testing\n",
    "results_nn_bc = lowprofool_generate_adversaries_test_nn(\n",
    "    lowprofool = lpf_neural_network_cancer,\n",
    "    classifier = nn_model_cancer, \n",
    "    x_valid    = X_valid_cancer, \n",
    "    y_valid    = y_valid_cancer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Original values ===\\n\")\n",
    "\n",
    "print_predictions(\n",
    "    cancer_scaler.inverse_transform(X_valid_cancer[-2:]),\n",
    "    neural_network_cancer_wrapper.predict(X_valid_cancer[-2:].values.astype(np.float32)),\n",
    "    max_features=30\n",
    ")\n",
    "\n",
    "print(\"\\n=== Adversaries (LowProFool results) ===\\n\")\n",
    "\n",
    "print_predictions(\n",
    "    cancer_scaler.inverse_transform(results_nn_bc[-2:]), \n",
    "    neural_network_cancer_wrapper.predict(results_nn_bc.astype(np.float32)[-2:]),\n",
    "    max_features=30\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('art-aml': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c317be1f25458f4932790ed219f1158e57c12adaecb215f67c10dfd49657577c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
