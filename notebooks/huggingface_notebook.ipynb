{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8093e27a-33f6-4cd9-a47b-ea94c3d0c514",
   "metadata": {},
   "source": [
    "# Huggingface with ART\n",
    "\n",
    "In this notebook we will go over how to use the Huggingface AIP with ART. This can enable us to train robust foundation models which act over images. \n",
    "\n",
    "Currently this is a developing feature, and so not all ART tools are supported. Further tools and development is planned. As of ART 1.16 we support: \n",
    "+ Using a Pytorch backend.\n",
    "+ Evasion attacks and defences on classical classification tasks such as image classification, but not tasks such as object detection.\n",
    "\n",
    "If you have a use case that is not supported (or find a bug in this new feature!) please raise an issiue on ART.\n",
    "\n",
    "Let's look at how we can use ART to secure Huggingface models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549129e5-f6f4-4995-b334-d22cb4cae76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giulio.zizzo1/anaconda3/envs/art_dev_1.16/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Relevant imports for the notebook\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from art.estimators.hugging_face import HuggingFaceClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adf2bba-efc1-4db7-8041-698105537b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use CIFAR data for the notebook.\n",
    "def get_cifar_data():\n",
    "    \"\"\"\n",
    "    Get CIFAR-10 data.\n",
    "    :return: cifar train/test data.\n",
    "    \"\"\"\n",
    "    train_set = datasets.CIFAR10('./data', train=True, download=True)\n",
    "    test_set = datasets.CIFAR10('./data', train=False, download=True)\n",
    "\n",
    "    x_train = train_set.data.astype(np.float32)\n",
    "    y_train = np.asarray(train_set.targets)\n",
    "\n",
    "    x_test = test_set.data.astype(np.float32)\n",
    "    y_test = np.asarray(test_set.targets)\n",
    "\n",
    "    x_train = np.moveaxis(x_train, [3], [1])\n",
    "    x_test = np.moveaxis(x_test, [3], [1])\n",
    "\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f797586-480b-432e-8b55-26a4ac2360f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_model():\n",
    "    \"\"\"\n",
    "    Train a cifar classifier\n",
    "    \"\"\"\n",
    "    from torch.optim import Adam\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = get_cifar_data()\n",
    "\n",
    "    # Here we load a Huggingface model using the transformers library.\n",
    "    model = transformers.AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224',\n",
    "                                                                         ignore_mismatched_sizes=True,\n",
    "                                                                         num_labels=10)\n",
    "\n",
    "    # The HuggingFaceClassifier follows broadly the same API as the PyTorchClassifier\n",
    "    # So we can supply the loss function, the input shape of the data we will supply, the optimizer, etc.\n",
    "    # Note, frequently we will be performing fine-tuning or transfer learning with vision transformners and \n",
    "    # so we may be fine-tuning on differently sized inputs. \n",
    "    # The input_shape argument refers to the shape of the supplied input data which may be different to \n",
    "    # the shape required by the model. \n",
    "    # To handle this HuggingFaceClassifier has an extra argument of processor which will act on \n",
    "    # every batch to process the data into the correct form required by the supplied model.\n",
    "    # This needs to be manually specified by the user. For many attacks and defences to work it needs to be a \n",
    "    # differentiable funciton. \n",
    "    # Here the processor is a simple upsampler to enlarge the cifar images into the right size.\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    upsampler = torch.nn.Upsample(scale_factor=7, mode='nearest')\n",
    "\n",
    "    hf_model = HuggingFaceClassifier(model, \n",
    "                                    loss=torch.nn.CrossEntropyLoss(),\n",
    "                                    optimizer=optimizer,\n",
    "                                    input_shape=(3, 32, 32),\n",
    "                                    nb_classes=10,\n",
    "                                    processor=upsampler)\n",
    "\n",
    "    hf_model.fit(x_train, y_train, nb_epochs=2, verbose=True)\n",
    "    torch.save(hf_model.model.state_dict(), 'hf_base_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a370096-d145-42cf-a59e-c553d81b0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pgd(model_to_test='hf_base_model.pt'):\n",
    "    \"\"\"\n",
    "    Here we can test the model we trained against a PGD attack.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
    "    (x_train, y_train), (x_test, y_test) = get_cifar_data()\n",
    "    model = transformers.AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224',\n",
    "                                                                         ignore_mismatched_sizes=True,\n",
    "                                                                         num_labels=10)\n",
    "    \n",
    "    # Load the model state dict from the training loop we just performed.\n",
    "    model.load_state_dict(torch.load(os.path.join('..', model_to_test)))\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Set it up as a HuggingFaceClassifier\n",
    "    hf_model = HuggingFaceClassifier(model, \n",
    "                                    loss=torch.nn.CrossEntropyLoss(),\n",
    "                                    optimizer=optimizer,\n",
    "                                    input_shape=(3, 32, 32),\n",
    "                                    nb_classes=10,\n",
    "                                    processor=torch.nn.Upsample(scale_factor=7, mode='nearest'))\n",
    "\n",
    "    # Let's just use 10 samples for quick demo purposes\n",
    "    num_samples = 10\n",
    "    outputs = hf_model(x_test[:num_samples])\n",
    "    acc = np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == y_test[:num_samples]) / len(y_test[:num_samples])\n",
    "    print('clean acc ', acc)\n",
    "\n",
    "    # The backend of the HuggingFaceClassifier is the existing PyTorchClassifier. Thus we can interface HuggingFaceClassifier with\n",
    "    # aleadry existing attacks in ART which support pytorch. Here we use ProjectedGradientDescentPyTorch.\n",
    "    attacker = ProjectedGradientDescentPyTorch(hf_model, eps=8/255, eps_step=1/255)\n",
    "    x_test_adv_robust = attacker.generate(x_test[:num_samples])\n",
    "    outputs = hf_model(x_test_adv_robust)\n",
    "    acc = np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == y_test[:num_samples]) / len(y_test[:num_samples])\n",
    "    print('adv acc ', acc)\n",
    "\n",
    "    # We can display the adversarial examples to highlight the added perturbation to the original sample.\n",
    "    x_test_adv_robust = np.moveaxis(x_test_adv_robust, [1], [3])\n",
    "    x_test = np.moveaxis(x_test, [1], [3])\n",
    "\n",
    "    delta = x_test[:num_samples] - x_test_adv_robust\n",
    "\n",
    "    delta = (delta - np.min(delta)) / (np.max(delta) - np.min(delta))\n",
    "    fig, axs = plt.subplots(3, 3)\n",
    "    for i in range(3):\n",
    "        axs[i, 0].imshow(x_test_adv_robust[i])\n",
    "        axs[i, 1].imshow(x_test[i])\n",
    "        axs[i, 2].imshow(delta[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4625ff-103f-49d2-90f9-503f1ea43e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTForImageClassification'> and id 140118286866448 and name  with submods 226\n",
      "found ViTModel(\n",
      "  (embeddings): ViTEmbeddings(\n",
      "    (patch_embeddings): ViTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): ViTEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x ViTLayer(\n",
      "        (attention): ViTAttention(\n",
      "          (attention): ViTSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): ViTSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): ViTIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): ViTOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTModel'> and id 140118349924048 and name vit with submods 224\n",
      "found ViTEmbeddings(\n",
      "  (patch_embeddings): ViTPatchEmbeddings(\n",
      "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTEmbeddings'> and id 140118349958672 and name vit.embeddings with submods 4\n",
      "found ViTPatchEmbeddings(\n",
      "  (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTPatchEmbeddings'> and id 140118284296208 and name vit.embeddings.patch_embeddings with submods 2\n",
      "found Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16)) with type <class 'torch.nn.modules.conv.Conv2d'> and id 140118328688336 and name vit.embeddings.patch_embeddings.projection with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118329994320 and name vit.embeddings.dropout with submods 1\n",
      "found ViTEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-11): 12 x ViTLayer(\n",
      "      (attention): ViTAttention(\n",
      "        (attention): ViTSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): ViTSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): ViTIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): ViTOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTEncoder'> and id 140118284304336 and name vit.encoder with submods 218\n",
      "found ModuleList(\n",
      "  (0-11): 12 x ViTLayer(\n",
      "    (attention): ViTAttention(\n",
      "      (attention): ViTSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (output): ViTSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): ViTIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): ViTOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ") with type <class 'torch.nn.modules.container.ModuleList'> and id 140118284170128 and name vit.encoder.layer with submods 217\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284298896 and name vit.encoder.layer.0 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284290768 and name vit.encoder.layer.0.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284298960 and name vit.encoder.layer.0.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118350319184 and name vit.encoder.layer.0.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284298512 and name vit.encoder.layer.0.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284298576 and name vit.encoder.layer.0.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284298128 and name vit.encoder.layer.0.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284297872 and name vit.encoder.layer.0.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118331679056 and name vit.encoder.layer.0.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284297424 and name vit.encoder.layer.0.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284297488 and name vit.encoder.layer.0.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140124252163920 and name vit.encoder.layer.0.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284296976 and name vit.encoder.layer.0.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284292560 and name vit.encoder.layer.0.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118329994768 and name vit.encoder.layer.0.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284296272 and name vit.encoder.layer.0.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284296464 and name vit.encoder.layer.0.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118329994512 and name vit.encoder.layer.0.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284295568 and name vit.encoder.layer.1 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284295504 and name vit.encoder.layer.1.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284295376 and name vit.encoder.layer.1.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284295312 and name vit.encoder.layer.1.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284294800 and name vit.encoder.layer.1.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284294864 and name vit.encoder.layer.1.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284294352 and name vit.encoder.layer.1.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284294032 and name vit.encoder.layer.1.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284293904 and name vit.encoder.layer.1.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284293584 and name vit.encoder.layer.1.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284293648 and name vit.encoder.layer.1.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284293456 and name vit.encoder.layer.1.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284292880 and name vit.encoder.layer.1.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284292496 and name vit.encoder.layer.1.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284292304 and name vit.encoder.layer.1.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284291920 and name vit.encoder.layer.1.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284291984 and name vit.encoder.layer.1.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284291472 and name vit.encoder.layer.1.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284290960 and name vit.encoder.layer.2 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284290704 and name vit.encoder.layer.2.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284290576 and name vit.encoder.layer.2.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284290512 and name vit.encoder.layer.2.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284289744 and name vit.encoder.layer.2.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284289936 and name vit.encoder.layer.2.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284289360 and name vit.encoder.layer.2.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284289104 and name vit.encoder.layer.2.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284288720 and name vit.encoder.layer.2.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284288208 and name vit.encoder.layer.2.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284288272 and name vit.encoder.layer.2.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118328810000 and name vit.encoder.layer.2.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284277456 and name vit.encoder.layer.2.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284282064 and name vit.encoder.layer.2.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284272720 and name vit.encoder.layer.2.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284272784 and name vit.encoder.layer.2.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284282448 and name vit.encoder.layer.2.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284278032 and name vit.encoder.layer.2.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284282000 and name vit.encoder.layer.3 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284275536 and name vit.encoder.layer.3.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284287888 and name vit.encoder.layer.3.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284287824 and name vit.encoder.layer.3.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284286864 and name vit.encoder.layer.3.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284286928 and name vit.encoder.layer.3.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284286096 and name vit.encoder.layer.3.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284285520 and name vit.encoder.layer.3.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284285392 and name vit.encoder.layer.3.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284284432 and name vit.encoder.layer.3.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284284496 and name vit.encoder.layer.3.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284284048 and name vit.encoder.layer.3.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284283280 and name vit.encoder.layer.3.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284283024 and name vit.encoder.layer.3.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284282640 and name vit.encoder.layer.3.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284281872 and name vit.encoder.layer.3.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284282192 and name vit.encoder.layer.3.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284281360 and name vit.encoder.layer.3.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284280912 and name vit.encoder.layer.4 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284280400 and name vit.encoder.layer.4.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284280016 and name vit.encoder.layer.4.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284279952 and name vit.encoder.layer.4.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284279056 and name vit.encoder.layer.4.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284279120 and name vit.encoder.layer.4.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284278288 and name vit.encoder.layer.4.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284277904 and name vit.encoder.layer.4.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284277648 and name vit.encoder.layer.4.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284276752 and name vit.encoder.layer.4.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284276944 and name vit.encoder.layer.4.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284276368 and name vit.encoder.layer.4.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284275344 and name vit.encoder.layer.4.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284274896 and name vit.encoder.layer.4.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284274832 and name vit.encoder.layer.4.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284274320 and name vit.encoder.layer.4.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284274384 and name vit.encoder.layer.4.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284273360 and name vit.encoder.layer.4.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284272336 and name vit.encoder.layer.5 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284272016 and name vit.encoder.layer.5.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284271952 and name vit.encoder.layer.5.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284271888 and name vit.encoder.layer.5.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284259216 and name vit.encoder.layer.5.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284255504 and name vit.encoder.layer.5.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284261264 and name vit.encoder.layer.5.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284258640 and name vit.encoder.layer.5.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284257552 and name vit.encoder.layer.5.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284259088 and name vit.encoder.layer.5.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284259920 and name vit.encoder.layer.5.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284266384 and name vit.encoder.layer.5.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284270992 and name vit.encoder.layer.5.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284270864 and name vit.encoder.layer.5.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284270544 and name vit.encoder.layer.5.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284269904 and name vit.encoder.layer.5.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284270096 and name vit.encoder.layer.5.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284269328 and name vit.encoder.layer.5.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284269200 and name vit.encoder.layer.6 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284268944 and name vit.encoder.layer.6.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284268816 and name vit.encoder.layer.6.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284268752 and name vit.encoder.layer.6.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284268368 and name vit.encoder.layer.6.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284268432 and name vit.encoder.layer.6.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284268112 and name vit.encoder.layer.6.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284267792 and name vit.encoder.layer.6.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284267664 and name vit.encoder.layer.6.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284267088 and name vit.encoder.layer.6.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284267152 and name vit.encoder.layer.6.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284266960 and name vit.encoder.layer.6.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284266320 and name vit.encoder.layer.6.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284265808 and name vit.encoder.layer.6.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284265360 and name vit.encoder.layer.6.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284264720 and name vit.encoder.layer.6.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284264912 and name vit.encoder.layer.6.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284264400 and name vit.encoder.layer.6.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284264144 and name vit.encoder.layer.7 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284264016 and name vit.encoder.layer.7.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284263888 and name vit.encoder.layer.7.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284263696 and name vit.encoder.layer.7.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284263184 and name vit.encoder.layer.7.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284263248 and name vit.encoder.layer.7.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284262736 and name vit.encoder.layer.7.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284262416 and name vit.encoder.layer.7.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284262160 and name vit.encoder.layer.7.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284261840 and name vit.encoder.layer.7.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284261904 and name vit.encoder.layer.7.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284261712 and name vit.encoder.layer.7.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284261136 and name vit.encoder.layer.7.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284260944 and name vit.encoder.layer.7.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284260496 and name vit.encoder.layer.7.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284260304 and name vit.encoder.layer.7.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284260368 and name vit.encoder.layer.7.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284259792 and name vit.encoder.layer.7.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284259408 and name vit.encoder.layer.8 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284259280 and name vit.encoder.layer.8.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284258960 and name vit.encoder.layer.8.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284258896 and name vit.encoder.layer.8.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284258000 and name vit.encoder.layer.8.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284258064 and name vit.encoder.layer.8.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284257296 and name vit.encoder.layer.8.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284256848 and name vit.encoder.layer.8.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284256720 and name vit.encoder.layer.8.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284256336 and name vit.encoder.layer.8.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284256400 and name vit.encoder.layer.8.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284256080 and name vit.encoder.layer.8.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118346082832 and name vit.encoder.layer.8.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118346082512 and name vit.encoder.layer.8.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140124252242704 and name vit.encoder.layer.8.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284393360 and name vit.encoder.layer.8.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284388816 and name vit.encoder.layer.8.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284392848 and name vit.encoder.layer.8.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284272912 and name vit.encoder.layer.9 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284392400 and name vit.encoder.layer.9.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284392464 and name vit.encoder.layer.9.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284392336 and name vit.encoder.layer.9.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284391952 and name vit.encoder.layer.9.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284392016 and name vit.encoder.layer.9.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284391696 and name vit.encoder.layer.9.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284391504 and name vit.encoder.layer.9.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284391120 and name vit.encoder.layer.9.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284390672 and name vit.encoder.layer.9.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284390736 and name vit.encoder.layer.9.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284390544 and name vit.encoder.layer.9.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284389840 and name vit.encoder.layer.9.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284389584 and name vit.encoder.layer.9.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284389392 and name vit.encoder.layer.9.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284389008 and name vit.encoder.layer.9.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284389072 and name vit.encoder.layer.9.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284388432 and name vit.encoder.layer.9.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284388176 and name vit.encoder.layer.10 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284388048 and name vit.encoder.layer.10.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284387920 and name vit.encoder.layer.10.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284387856 and name vit.encoder.layer.10.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284387344 and name vit.encoder.layer.10.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284387536 and name vit.encoder.layer.10.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284386960 and name vit.encoder.layer.10.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284386640 and name vit.encoder.layer.10.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284386512 and name vit.encoder.layer.10.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118285056208 and name vit.encoder.layer.10.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118285056464 and name vit.encoder.layer.10.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118285056720 and name vit.encoder.layer.10.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284164176 and name vit.encoder.layer.10.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284166224 and name vit.encoder.layer.10.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284163856 and name vit.encoder.layer.10.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284163664 and name vit.encoder.layer.10.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284163088 and name vit.encoder.layer.10.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284169040 and name vit.encoder.layer.10.layernorm_after with submods 1\n",
      "found ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTLayer'> and id 140118284163536 and name vit.encoder.layer.11 with submods 18\n",
      "found ViTAttention(\n",
      "  (attention): ViTSelfAttention(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output): ViTSelfOutput(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTAttention'> and id 140118284162960 and name vit.encoder.layer.11.attention with submods 9\n",
      "found ViTSelfAttention(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfAttention'> and id 140118284161104 and name vit.encoder.layer.11.attention.attention with submods 5\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284172496 and name vit.encoder.layer.11.attention.attention.query with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284158096 and name vit.encoder.layer.11.attention.attention.key with submods 1\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284158672 and name vit.encoder.layer.11.attention.attention.value with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284170320 and name vit.encoder.layer.11.attention.attention.dropout with submods 1\n",
      "found ViTSelfOutput(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTSelfOutput'> and id 140118284166992 and name vit.encoder.layer.11.attention.output with submods 3\n",
      "found Linear(in_features=768, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284165776 and name vit.encoder.layer.11.attention.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284162576 and name vit.encoder.layer.11.attention.output.dropout with submods 1\n",
      "found ViTIntermediate(\n",
      "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTIntermediate'> and id 140118284162768 and name vit.encoder.layer.11.intermediate with submods 3\n",
      "found Linear(in_features=768, out_features=3072, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284157968 and name vit.encoder.layer.11.intermediate.dense with submods 1\n",
      "found GELUActivation() with type <class 'transformers.activations.GELUActivation'> and id 140118284172432 and name vit.encoder.layer.11.intermediate.intermediate_act_fn with submods 1\n",
      "found ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ") with type <class 'transformers.models.vit.modeling_vit.ViTOutput'> and id 140118284172304 and name vit.encoder.layer.11.output with submods 3\n",
      "found Linear(in_features=3072, out_features=768, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284171856 and name vit.encoder.layer.11.output.dense with submods 1\n",
      "found Dropout(p=0.0, inplace=False) with type <class 'torch.nn.modules.dropout.Dropout'> and id 140118284171152 and name vit.encoder.layer.11.output.dropout with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284171472 and name vit.encoder.layer.11.layernorm_before with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118284170576 and name vit.encoder.layer.11.layernorm_after with submods 1\n",
      "found LayerNorm((768,), eps=1e-12, elementwise_affine=True) with type <class 'torch.nn.modules.normalization.LayerNorm'> and id 140118329994448 and name vit.layernorm with submods 1\n",
      "found Linear(in_features=768, out_features=10, bias=True) with type <class 'torch.nn.modules.linear.Linear'> and id 140118284168848 and name classifier with submods 1\n",
      "\n",
      "\n",
      "mapping from id to name is  {140118328688336: 'vit.embeddings.patch_embeddings.projection', 140118329994320: 'vit.embeddings.dropout', 140118350319184: 'vit.encoder.layer.0.attention.attention.query', 140118284298512: 'vit.encoder.layer.0.attention.attention.key', 140118284298576: 'vit.encoder.layer.0.attention.attention.value', 140118284298128: 'vit.encoder.layer.0.attention.attention.dropout', 140118331679056: 'vit.encoder.layer.0.attention.output.dense', 140118284297424: 'vit.encoder.layer.0.attention.output.dropout', 140124252163920: 'vit.encoder.layer.0.intermediate.dense', 140118284296976: 'vit.encoder.layer.0.intermediate.intermediate_act_fn', 140118329994768: 'vit.encoder.layer.0.output.dense', 140118284296272: 'vit.encoder.layer.0.output.dropout', 140118284296464: 'vit.encoder.layer.0.layernorm_before', 140118329994512: 'vit.encoder.layer.0.layernorm_after', 140118284295312: 'vit.encoder.layer.1.attention.attention.query', 140118284294800: 'vit.encoder.layer.1.attention.attention.key', 140118284294864: 'vit.encoder.layer.1.attention.attention.value', 140118284294352: 'vit.encoder.layer.1.attention.attention.dropout', 140118284293904: 'vit.encoder.layer.1.attention.output.dense', 140118284293584: 'vit.encoder.layer.1.attention.output.dropout', 140118284293456: 'vit.encoder.layer.1.intermediate.dense', 140118284292880: 'vit.encoder.layer.1.intermediate.intermediate_act_fn', 140118284292304: 'vit.encoder.layer.1.output.dense', 140118284291920: 'vit.encoder.layer.1.output.dropout', 140118284291984: 'vit.encoder.layer.1.layernorm_before', 140118284291472: 'vit.encoder.layer.1.layernorm_after', 140118284290512: 'vit.encoder.layer.2.attention.attention.query', 140118284289744: 'vit.encoder.layer.2.attention.attention.key', 140118284289936: 'vit.encoder.layer.2.attention.attention.value', 140118284289360: 'vit.encoder.layer.2.attention.attention.dropout', 140118284288720: 'vit.encoder.layer.2.attention.output.dense', 140118284288208: 'vit.encoder.layer.2.attention.output.dropout', 140118328810000: 'vit.encoder.layer.2.intermediate.dense', 140118284277456: 'vit.encoder.layer.2.intermediate.intermediate_act_fn', 140118284272720: 'vit.encoder.layer.2.output.dense', 140118284272784: 'vit.encoder.layer.2.output.dropout', 140118284282448: 'vit.encoder.layer.2.layernorm_before', 140118284278032: 'vit.encoder.layer.2.layernorm_after', 140118284287824: 'vit.encoder.layer.3.attention.attention.query', 140118284286864: 'vit.encoder.layer.3.attention.attention.key', 140118284286928: 'vit.encoder.layer.3.attention.attention.value', 140118284286096: 'vit.encoder.layer.3.attention.attention.dropout', 140118284285392: 'vit.encoder.layer.3.attention.output.dense', 140118284284432: 'vit.encoder.layer.3.attention.output.dropout', 140118284284048: 'vit.encoder.layer.3.intermediate.dense', 140118284283280: 'vit.encoder.layer.3.intermediate.intermediate_act_fn', 140118284282640: 'vit.encoder.layer.3.output.dense', 140118284281872: 'vit.encoder.layer.3.output.dropout', 140118284282192: 'vit.encoder.layer.3.layernorm_before', 140118284281360: 'vit.encoder.layer.3.layernorm_after', 140118284279952: 'vit.encoder.layer.4.attention.attention.query', 140118284279056: 'vit.encoder.layer.4.attention.attention.key', 140118284279120: 'vit.encoder.layer.4.attention.attention.value', 140118284278288: 'vit.encoder.layer.4.attention.attention.dropout', 140118284277648: 'vit.encoder.layer.4.attention.output.dense', 140118284276752: 'vit.encoder.layer.4.attention.output.dropout', 140118284276368: 'vit.encoder.layer.4.intermediate.dense', 140118284275344: 'vit.encoder.layer.4.intermediate.intermediate_act_fn', 140118284274832: 'vit.encoder.layer.4.output.dense', 140118284274320: 'vit.encoder.layer.4.output.dropout', 140118284274384: 'vit.encoder.layer.4.layernorm_before', 140118284273360: 'vit.encoder.layer.4.layernorm_after', 140118284271888: 'vit.encoder.layer.5.attention.attention.query', 140118284259216: 'vit.encoder.layer.5.attention.attention.key', 140118284255504: 'vit.encoder.layer.5.attention.attention.value', 140118284261264: 'vit.encoder.layer.5.attention.attention.dropout', 140118284257552: 'vit.encoder.layer.5.attention.output.dense', 140118284259088: 'vit.encoder.layer.5.attention.output.dropout', 140118284266384: 'vit.encoder.layer.5.intermediate.dense', 140118284270992: 'vit.encoder.layer.5.intermediate.intermediate_act_fn', 140118284270544: 'vit.encoder.layer.5.output.dense', 140118284269904: 'vit.encoder.layer.5.output.dropout', 140118284270096: 'vit.encoder.layer.5.layernorm_before', 140118284269328: 'vit.encoder.layer.5.layernorm_after', 140118284268752: 'vit.encoder.layer.6.attention.attention.query', 140118284268368: 'vit.encoder.layer.6.attention.attention.key', 140118284268432: 'vit.encoder.layer.6.attention.attention.value', 140118284268112: 'vit.encoder.layer.6.attention.attention.dropout', 140118284267664: 'vit.encoder.layer.6.attention.output.dense', 140118284267088: 'vit.encoder.layer.6.attention.output.dropout', 140118284266960: 'vit.encoder.layer.6.intermediate.dense', 140118284266320: 'vit.encoder.layer.6.intermediate.intermediate_act_fn', 140118284265360: 'vit.encoder.layer.6.output.dense', 140118284264720: 'vit.encoder.layer.6.output.dropout', 140118284264912: 'vit.encoder.layer.6.layernorm_before', 140118284264400: 'vit.encoder.layer.6.layernorm_after', 140118284263696: 'vit.encoder.layer.7.attention.attention.query', 140118284263184: 'vit.encoder.layer.7.attention.attention.key', 140118284263248: 'vit.encoder.layer.7.attention.attention.value', 140118284262736: 'vit.encoder.layer.7.attention.attention.dropout', 140118284262160: 'vit.encoder.layer.7.attention.output.dense', 140118284261840: 'vit.encoder.layer.7.attention.output.dropout', 140118284261712: 'vit.encoder.layer.7.intermediate.dense', 140118284261136: 'vit.encoder.layer.7.intermediate.intermediate_act_fn', 140118284260496: 'vit.encoder.layer.7.output.dense', 140118284260304: 'vit.encoder.layer.7.output.dropout', 140118284260368: 'vit.encoder.layer.7.layernorm_before', 140118284259792: 'vit.encoder.layer.7.layernorm_after', 140118284258896: 'vit.encoder.layer.8.attention.attention.query', 140118284258000: 'vit.encoder.layer.8.attention.attention.key', 140118284258064: 'vit.encoder.layer.8.attention.attention.value', 140118284257296: 'vit.encoder.layer.8.attention.attention.dropout', 140118284256720: 'vit.encoder.layer.8.attention.output.dense', 140118284256336: 'vit.encoder.layer.8.attention.output.dropout', 140118284256080: 'vit.encoder.layer.8.intermediate.dense', 140118346082832: 'vit.encoder.layer.8.intermediate.intermediate_act_fn', 140124252242704: 'vit.encoder.layer.8.output.dense', 140118284393360: 'vit.encoder.layer.8.output.dropout', 140118284388816: 'vit.encoder.layer.8.layernorm_before', 140118284392848: 'vit.encoder.layer.8.layernorm_after', 140118284392336: 'vit.encoder.layer.9.attention.attention.query', 140118284391952: 'vit.encoder.layer.9.attention.attention.key', 140118284392016: 'vit.encoder.layer.9.attention.attention.value', 140118284391696: 'vit.encoder.layer.9.attention.attention.dropout', 140118284391120: 'vit.encoder.layer.9.attention.output.dense', 140118284390672: 'vit.encoder.layer.9.attention.output.dropout', 140118284390544: 'vit.encoder.layer.9.intermediate.dense', 140118284389840: 'vit.encoder.layer.9.intermediate.intermediate_act_fn', 140118284389392: 'vit.encoder.layer.9.output.dense', 140118284389008: 'vit.encoder.layer.9.output.dropout', 140118284389072: 'vit.encoder.layer.9.layernorm_before', 140118284388432: 'vit.encoder.layer.9.layernorm_after', 140118284387856: 'vit.encoder.layer.10.attention.attention.query', 140118284387344: 'vit.encoder.layer.10.attention.attention.key', 140118284387536: 'vit.encoder.layer.10.attention.attention.value', 140118284386960: 'vit.encoder.layer.10.attention.attention.dropout', 140118284386512: 'vit.encoder.layer.10.attention.output.dense', 140118285056208: 'vit.encoder.layer.10.attention.output.dropout', 140118285056720: 'vit.encoder.layer.10.intermediate.dense', 140118284164176: 'vit.encoder.layer.10.intermediate.intermediate_act_fn', 140118284163856: 'vit.encoder.layer.10.output.dense', 140118284163664: 'vit.encoder.layer.10.output.dropout', 140118284163088: 'vit.encoder.layer.10.layernorm_before', 140118284169040: 'vit.encoder.layer.10.layernorm_after', 140118284172496: 'vit.encoder.layer.11.attention.attention.query', 140118284158096: 'vit.encoder.layer.11.attention.attention.key', 140118284158672: 'vit.encoder.layer.11.attention.attention.value', 140118284170320: 'vit.encoder.layer.11.attention.attention.dropout', 140118284165776: 'vit.encoder.layer.11.attention.output.dense', 140118284162576: 'vit.encoder.layer.11.attention.output.dropout', 140118284157968: 'vit.encoder.layer.11.intermediate.dense', 140118284172432: 'vit.encoder.layer.11.intermediate.intermediate_act_fn', 140118284171856: 'vit.encoder.layer.11.output.dense', 140118284171152: 'vit.encoder.layer.11.output.dropout', 140118284171472: 'vit.encoder.layer.11.layernorm_before', 140118284170576: 'vit.encoder.layer.11.layernorm_after', 140118329994448: 'vit.layernorm', 140118284168848: 'classifier'}\n",
      "------ Finished Registering Hooks------\n",
      "input_module is Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16)) with id 140118328688336\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118329994320\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284296464\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118350319184\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284298512\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284298576\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284298128\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118331679056\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284297424\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118329994512\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140124252163920\n",
      "input_module is GELUActivation() with id 140118284296976\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118329994768\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284296272\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284291984\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284295312\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284294800\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284294864\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284294352\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284293904\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284293584\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284291472\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284293456\n",
      "input_module is GELUActivation() with id 140118284292880\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284292304\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284291920\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284282448\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284290512\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284289744\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284289936\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284289360\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284288720\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284288208\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284278032\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118328810000\n",
      "input_module is GELUActivation() with id 140118284277456\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284272720\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284272784\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284282192\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284287824\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284286864\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284286928\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284286096\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284285392\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284284432\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284281360\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284284048\n",
      "input_module is GELUActivation() with id 140118284283280\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284282640\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284281872\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284274384\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284279952\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284279056\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284279120\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284278288\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284277648\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284276752\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284273360\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284276368\n",
      "input_module is GELUActivation() with id 140118284275344\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284274832\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284274320\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284270096\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284271888\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284259216\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284255504\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284261264\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284257552\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284259088\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284269328\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284266384\n",
      "input_module is GELUActivation() with id 140118284270992\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284270544\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284269904\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284264912\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284268752\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284268368\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284268432\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284268112\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284267664\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284267088\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284264400\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284266960\n",
      "input_module is GELUActivation() with id 140118284266320\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284265360\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284264720\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284260368\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284263696\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284263184\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284263248\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284262736\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284262160\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284261840\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284259792\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284261712\n",
      "input_module is GELUActivation() with id 140118284261136\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284260496\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284260304\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284388816\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284258896\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284258000\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284258064\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284257296\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284256720\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284256336\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284392848\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284256080\n",
      "input_module is GELUActivation() with id 140118346082832\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140124252242704\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284393360\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284389072\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284392336\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284391952\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284392016\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284391696\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284391120\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284390672\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284388432\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284390544\n",
      "input_module is GELUActivation() with id 140118284389840\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284389392\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284389008\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284163088\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284387856\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284387344\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284387536\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284386960\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284386512\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118285056208\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284169040\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118285056720\n",
      "input_module is GELUActivation() with id 140118284164176\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284163856\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284163664\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284171472\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284172496\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284158096\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284158672\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284170320\n",
      "input_module is Linear(in_features=768, out_features=768, bias=True) with id 140118284165776\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284162576\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118284170576\n",
      "input_module is Linear(in_features=768, out_features=3072, bias=True) with id 140118284157968\n",
      "input_module is GELUActivation() with id 140118284172432\n",
      "input_module is Linear(in_features=3072, out_features=768, bias=True) with id 140118284171856\n",
      "input_module is Dropout(p=0.0, inplace=False) with id 140118284171152\n",
      "input_module is LayerNorm((768,), eps=1e-12, elementwise_affine=True) with id 140118329994448\n",
      "input_module is Linear(in_features=768, out_features=10, bias=True) with id 140118284168848\n",
      "------ Finished Fire Hooks------\n",
      "new result is \n",
      "['vit.embeddings.patch_embeddings.projection', 'vit.embeddings.dropout', 'vit.encoder.layer.0.layernorm_before', 'vit.encoder.layer.0.attention.attention.query', 'vit.encoder.layer.0.attention.attention.key', 'vit.encoder.layer.0.attention.attention.value', 'vit.encoder.layer.0.attention.attention.dropout', 'vit.encoder.layer.0.attention.output.dense', 'vit.encoder.layer.0.attention.output.dropout', 'vit.encoder.layer.0.layernorm_after', 'vit.encoder.layer.0.intermediate.dense', 'vit.encoder.layer.0.intermediate.intermediate_act_fn', 'vit.encoder.layer.0.output.dense', 'vit.encoder.layer.0.output.dropout', 'vit.encoder.layer.1.layernorm_before', 'vit.encoder.layer.1.attention.attention.query', 'vit.encoder.layer.1.attention.attention.key', 'vit.encoder.layer.1.attention.attention.value', 'vit.encoder.layer.1.attention.attention.dropout', 'vit.encoder.layer.1.attention.output.dense', 'vit.encoder.layer.1.attention.output.dropout', 'vit.encoder.layer.1.layernorm_after', 'vit.encoder.layer.1.intermediate.dense', 'vit.encoder.layer.1.intermediate.intermediate_act_fn', 'vit.encoder.layer.1.output.dense', 'vit.encoder.layer.1.output.dropout', 'vit.encoder.layer.2.layernorm_before', 'vit.encoder.layer.2.attention.attention.query', 'vit.encoder.layer.2.attention.attention.key', 'vit.encoder.layer.2.attention.attention.value', 'vit.encoder.layer.2.attention.attention.dropout', 'vit.encoder.layer.2.attention.output.dense', 'vit.encoder.layer.2.attention.output.dropout', 'vit.encoder.layer.2.layernorm_after', 'vit.encoder.layer.2.intermediate.dense', 'vit.encoder.layer.2.intermediate.intermediate_act_fn', 'vit.encoder.layer.2.output.dense', 'vit.encoder.layer.2.output.dropout', 'vit.encoder.layer.3.layernorm_before', 'vit.encoder.layer.3.attention.attention.query', 'vit.encoder.layer.3.attention.attention.key', 'vit.encoder.layer.3.attention.attention.value', 'vit.encoder.layer.3.attention.attention.dropout', 'vit.encoder.layer.3.attention.output.dense', 'vit.encoder.layer.3.attention.output.dropout', 'vit.encoder.layer.3.layernorm_after', 'vit.encoder.layer.3.intermediate.dense', 'vit.encoder.layer.3.intermediate.intermediate_act_fn', 'vit.encoder.layer.3.output.dense', 'vit.encoder.layer.3.output.dropout', 'vit.encoder.layer.4.layernorm_before', 'vit.encoder.layer.4.attention.attention.query', 'vit.encoder.layer.4.attention.attention.key', 'vit.encoder.layer.4.attention.attention.value', 'vit.encoder.layer.4.attention.attention.dropout', 'vit.encoder.layer.4.attention.output.dense', 'vit.encoder.layer.4.attention.output.dropout', 'vit.encoder.layer.4.layernorm_after', 'vit.encoder.layer.4.intermediate.dense', 'vit.encoder.layer.4.intermediate.intermediate_act_fn', 'vit.encoder.layer.4.output.dense', 'vit.encoder.layer.4.output.dropout', 'vit.encoder.layer.5.layernorm_before', 'vit.encoder.layer.5.attention.attention.query', 'vit.encoder.layer.5.attention.attention.key', 'vit.encoder.layer.5.attention.attention.value', 'vit.encoder.layer.5.attention.attention.dropout', 'vit.encoder.layer.5.attention.output.dense', 'vit.encoder.layer.5.attention.output.dropout', 'vit.encoder.layer.5.layernorm_after', 'vit.encoder.layer.5.intermediate.dense', 'vit.encoder.layer.5.intermediate.intermediate_act_fn', 'vit.encoder.layer.5.output.dense', 'vit.encoder.layer.5.output.dropout', 'vit.encoder.layer.6.layernorm_before', 'vit.encoder.layer.6.attention.attention.query', 'vit.encoder.layer.6.attention.attention.key', 'vit.encoder.layer.6.attention.attention.value', 'vit.encoder.layer.6.attention.attention.dropout', 'vit.encoder.layer.6.attention.output.dense', 'vit.encoder.layer.6.attention.output.dropout', 'vit.encoder.layer.6.layernorm_after', 'vit.encoder.layer.6.intermediate.dense', 'vit.encoder.layer.6.intermediate.intermediate_act_fn', 'vit.encoder.layer.6.output.dense', 'vit.encoder.layer.6.output.dropout', 'vit.encoder.layer.7.layernorm_before', 'vit.encoder.layer.7.attention.attention.query', 'vit.encoder.layer.7.attention.attention.key', 'vit.encoder.layer.7.attention.attention.value', 'vit.encoder.layer.7.attention.attention.dropout', 'vit.encoder.layer.7.attention.output.dense', 'vit.encoder.layer.7.attention.output.dropout', 'vit.encoder.layer.7.layernorm_after', 'vit.encoder.layer.7.intermediate.dense', 'vit.encoder.layer.7.intermediate.intermediate_act_fn', 'vit.encoder.layer.7.output.dense', 'vit.encoder.layer.7.output.dropout', 'vit.encoder.layer.8.layernorm_before', 'vit.encoder.layer.8.attention.attention.query', 'vit.encoder.layer.8.attention.attention.key', 'vit.encoder.layer.8.attention.attention.value', 'vit.encoder.layer.8.attention.attention.dropout', 'vit.encoder.layer.8.attention.output.dense', 'vit.encoder.layer.8.attention.output.dropout', 'vit.encoder.layer.8.layernorm_after', 'vit.encoder.layer.8.intermediate.dense', 'vit.encoder.layer.8.intermediate.intermediate_act_fn', 'vit.encoder.layer.8.output.dense', 'vit.encoder.layer.8.output.dropout', 'vit.encoder.layer.9.layernorm_before', 'vit.encoder.layer.9.attention.attention.query', 'vit.encoder.layer.9.attention.attention.key', 'vit.encoder.layer.9.attention.attention.value', 'vit.encoder.layer.9.attention.attention.dropout', 'vit.encoder.layer.9.attention.output.dense', 'vit.encoder.layer.9.attention.output.dropout', 'vit.encoder.layer.9.layernorm_after', 'vit.encoder.layer.9.intermediate.dense', 'vit.encoder.layer.9.intermediate.intermediate_act_fn', 'vit.encoder.layer.9.output.dense', 'vit.encoder.layer.9.output.dropout', 'vit.encoder.layer.10.layernorm_before', 'vit.encoder.layer.10.attention.attention.query', 'vit.encoder.layer.10.attention.attention.key', 'vit.encoder.layer.10.attention.attention.value', 'vit.encoder.layer.10.attention.attention.dropout', 'vit.encoder.layer.10.attention.output.dense', 'vit.encoder.layer.10.attention.output.dropout', 'vit.encoder.layer.10.layernorm_after', 'vit.encoder.layer.10.intermediate.dense', 'vit.encoder.layer.10.intermediate.intermediate_act_fn', 'vit.encoder.layer.10.output.dense', 'vit.encoder.layer.10.output.dropout', 'vit.encoder.layer.11.layernorm_before', 'vit.encoder.layer.11.attention.attention.query', 'vit.encoder.layer.11.attention.attention.key', 'vit.encoder.layer.11.attention.attention.value', 'vit.encoder.layer.11.attention.attention.dropout', 'vit.encoder.layer.11.attention.output.dense', 'vit.encoder.layer.11.attention.output.dropout', 'vit.encoder.layer.11.layernorm_after', 'vit.encoder.layer.11.intermediate.dense', 'vit.encoder.layer.11.intermediate.intermediate_act_fn', 'vit.encoder.layer.11.output.dense', 'vit.encoder.layer.11.output.dropout', 'vit.layernorm', 'classifier']\n",
      "clean acc  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).                                                                                           \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv acc  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81f918-e716-457d-a665-835542cf5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that we can attack the Huggingface transformer, so now let's use one of the defences in ART!\n",
    "\n",
    "def adversarial_train():\n",
    "    from art.defences.trainer import AdversarialTrainerMadryPGD\n",
    "    (x_train, y_train), (x_test, y_test) = get_cifar_data()\n",
    "    model = transformers.AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224',\n",
    "                                                                         ignore_mismatched_sizes=True,\n",
    "                                                                         num_labels=10)\n",
    "\n",
    "    upsampler = torch.nn.Upsample(scale_factor=7, mode='nearest')\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    hf_model = HuggingFaceClassifier(model, \n",
    "                                    loss=torch.nn.CrossEntropyLoss(), \n",
    "                                    input_shape=(3, 32, 32),\n",
    "                                    nb_classes=10,\n",
    "                                    optimizer=optimizer, \n",
    "                                    processor=upsampler)\n",
    "\n",
    "    # We can now use adversarial training with Madry's protocol.\n",
    "    trainer = AdversarialTrainerMadryPGD(hf_model,\n",
    "                                         nb_epochs=10,\n",
    "                                         eps=8/255,\n",
    "                                         eps_step=1/255,\n",
    "                                         max_iter=10)\n",
    "\n",
    "    trainer.fit(x_train, y_train, display_progress_bar=True)\n",
    "    torch.save(trainer._classifier.model.state_dict(), 'hf_adv_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76373745-f886-4f1f-9452-ec648753b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below to run the adverarial training, it can take some time depending on available hardware. \n",
    "# The expected runtime is around 15 hours using a Nvidia V100 GPU.\n",
    "\n",
    "# adversarial_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c859e2-c18a-4598-999b-b7621772a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test the adversariallty trained model and we can see we have done from 0% robustness to XXX%.\n",
    "test_pgd(model_to_test='hf_adv_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
