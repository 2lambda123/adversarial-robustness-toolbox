{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58063edd",
   "metadata": {},
   "source": [
    "# Certification of Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438abb9",
   "metadata": {},
   "source": [
    "In this notebook we will go over how to use the PyTorchSmoothedViT tool and be able to certify vision transformers against patch attacks!\n",
    "\n",
    "### Overview\n",
    "\n",
    "This was introduced in Certified Patch Robustness via Smoothed Vision Transformers (https://arxiv.org/abs/2110.07719). The core technique is one of *image ablations*, where the image is blanked out except for certain regions. By ablating the input in different ways every time we can obtain many predicitons for a single input. Now, as we are ablating large parts of the image the attacker's patch attack is also getting removed in many predictions. Based on factors like the size of the adversarial patch and the size of the retained part of the image the attacker will only be able to influence a limited number of predictions. In fact, if the attacker has a m x m patch attack and the retained part of the image is a column of width s then the maximum number of predictions that could be affected are: \n",
    "\n",
    "$\n",
    "insert equation here\n",
    "$\n",
    "\n",
    "Based on this relationship we can derive a simple but effective criterion that if we are making many predictions for an image and the highest predicted class $c_t$ has been predicted $k$ times and the second most predicted class $c_{t-1}$ has been predicted $k_{t-1}$ times then we have a certified prediction if: \n",
    "\n",
    "\n",
    "$insert here$\n",
    "\n",
    "Intuitivly we are saying that even if $k$ predictions were adversarially influenced and those predictions were to change, then the model will *still* have predicted class $c_t$.\n",
    "\n",
    "### What's special about Vision Transformers?\n",
    "\n",
    "The formulation above is very generic and it can be applied to any nerual network model, in fact the original paper which proposed it () considered the case with convolutional nerual networks. \n",
    "\n",
    "However, Vision Transformers (or ViTs) are well siuted to this task of predicting with vision ablations for two key reasons: \n",
    "\n",
    "+ ViTs first tokenize the input into a series of image regions which get embedded and then processed through the neural network. Thus, by considering the input as a set of tokens we can drop tokens which correspond to fully masked (i.e ablated)regions significantly saving on the compute costs. \n",
    "\n",
    "+ Secondly, the ViT's self attention layer enables sharing of information globally at every layer. In contrast convolutional neural networks build up the receptive field over a series of layers. Hence, ViTs can be more effective at classifying an image based on its small unablated regions.\n",
    "\n",
    "Let's see how to use these tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb27667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core tool is PyTorchSmoothedViT which can be imported as follows:\n",
    "from art.estimators.certification.smoothed_vision_transformers import PyTorchSmoothedViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ef5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few ways we can interface with it. \n",
    "# The most direct way to get setup is by specifying the name of a supported transformer.\n",
    "# Behind the scenes we are using the timm library (link: ) so any ViT supported by that libary will work.\n",
    "\n",
    "art_model = PyTorchSmoothedViT(model='vit_small_patch16_224', # Name of the model acitecture to load\n",
    "                               loss=torch.nn.CrossEntropyLoss(), # loss function to use\n",
    "                               optimizer=torch.optim.SGD, # the optimizer to use: note! this is not initialised here we just supply the class!\n",
    "                               optimizer_params={\"lr\": 0.01}, # the parameters to use\n",
    "                               input_shape=(3, 32, 32), # the input shape of the data: Note! ...\n",
    "                               nb_classes=10,\n",
    "                               ablation_size=4,\n",
    "                               load_pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4255f",
   "metadata": {},
   "source": [
    "Creating a PyTorchSmoothedViT instance with the above code follows many of the general ART patterns with two caveats: \n",
    "+ The optimizer would (normally) be supplied initialised into the estimator along with a pytorch model. However, here we have not yet created the model, we are just supplying the model architecture name. Hence, here we pass the class into PyTorchSmoothedViT with the keyword arguments in optimizer_params which you would normally use to initialise it.\n",
    "+ The input shape..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7253ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
